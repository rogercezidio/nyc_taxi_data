{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f79a83-913d-45e3-93b1-9ba7ef510f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from datetime import datetime\n",
    "from functools   import reduce\n",
    "from helpers import SILVER, GOLD, TAXI_TYPES, table_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74c7347-ae72-42eb-bd45-7bf8f250717b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìÖ Cria√ß√£o e populariza√ß√£o da dimens√£o de datas (`dim_date`)\n",
    "\n",
    "Este trecho cria e popula uma tabela de dimens√£o temporal com datas de **01/01/2023 a 31/12/2023**, contendo:\n",
    "\n",
    "- `date_id`: formato `yyyymmdd`;\n",
    "- Atributos como ano, m√™s, dia, dia da semana, quarter e flag de fim de semana.\n",
    "\n",
    "üéØ **Objetivo**: fornecer uma dimens√£o temporal para facilitar an√°lises agregadas e permitir *joins* eficientes com os dados de corridas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79369e65-a3ae-487e-923b-48bc5b968e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {GOLD.TBL_DIM_DATE} (\n",
    "    date_id      INT    COMMENT 'yyyymmdd',\n",
    "    date         DATE,\n",
    "    year         INT,\n",
    "    month        INT,\n",
    "    day          INT,\n",
    "    day_of_week  INT,          \n",
    "    is_weekend   BOOLEAN,\n",
    "    quarter      INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "dates = (\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT explode(\n",
    "                 sequence( to_date('2023-01-01')\n",
    "                         , to_date('2023-12-31')\n",
    "                         , interval 1 day)\n",
    "               ) AS date\n",
    "    \"\"\")\n",
    "    .withColumn(\"date_id\",     F.date_format(\"date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"year\",        F.year(\"date\"))\n",
    "    .withColumn(\"month\",       F.month(\"date\"))\n",
    "    .withColumn(\"day\",         F.dayofmonth(\"date\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\"))     \n",
    "    .withColumn(\"is_weekend\",  F.col(\"day_of_week\").isin(1,7))\n",
    "    .withColumn(\"quarter\",     F.quarter(\"date\"))\n",
    ")\n",
    "\n",
    "\n",
    "(dates.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")         \n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .saveAsTable(GOLD.TBL_DIM_DATE))       \n",
    "\n",
    "rows = spark.table(GOLD.TBL_DIM_DATE).count()\n",
    "print(f\"dim_date populada com {rows:,} linhas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514aacb0-8907-43e5-8db3-9481a1bd6d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üèÅ Cria√ß√£o da tabela fato `fact_trips` (camada Gold)\n",
    "\n",
    "Este trecho:\n",
    "\n",
    "- L√™ os dados da camada Silver para cada tipo de t√°xi;\n",
    "- Calcula colunas auxiliares como:\n",
    "  - `pickup_date_id` e `pickup_time_id` para facilitar an√°lises temporais;\n",
    "  - `duration_min`, `distance_mi` e `fare` para enriquecer a base;\n",
    "- Realiza **agrega√ß√µes por data e hora do embarque**, gerando m√©tricas como:\n",
    "  - `num_trips`, `total_revenue`, `total_passengers`, `avg_fare`, `revenue_mi`, etc;\n",
    "- Escreve o resultado final como tabela Delta particionada por `pickup_date_id`.\n",
    "\n",
    "üéØ **Objetivo**: consolidar uma vis√£o anal√≠tica unificada das corridas para suportar dashboards, KPIs e an√°lises diversas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9409dd2f-eb0d-4265-aaf3-bf468cbc8a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_dfs = []\n",
    "for tt in TAXI_TYPES:\n",
    "    src_tbl = f\"{SILVER.SCHEMA}.{tt}_tripdata_silver\"\n",
    "    if not spark.catalog.tableExists(src_tbl):\n",
    "        print(f\"Silver ausente ‚Üí {tt}\")\n",
    "        continue\n",
    "\n",
    "    df = spark.table(src_tbl)\n",
    "\n",
    "    pickup_col  = next(c for c in df.columns if c.endswith(\"pickup_datetime\"))\n",
    "    dropoff_col = next(c for c in df.columns if c.endswith(\"dropoff_datetime\"))\n",
    "\n",
    "    df = (df\n",
    "          .withColumnRenamed(pickup_col,  \"pickup_ts\")\n",
    "          .withColumnRenamed(dropoff_col, \"dropoff_ts\")\n",
    "          .withColumn(\"pickup_date_id\",  F.date_format(\"pickup_ts\", \"yyyyMMdd\").cast(\"int\"))\n",
    "          .withColumn(\"pickup_time_id\",  (F.hour(\"pickup_ts\")*100 + F.minute(\"pickup_ts\")).cast(\"int\"))\n",
    "          .withColumn(\"taxi_type\",       F.lit(tt))\n",
    "          .withColumn(\n",
    "            \"duration_min\",\n",
    "            F.expr(\"timestampdiff(SECOND, pickup_ts, dropoff_ts)\") / 60.0\n",
    "           )\n",
    "          .withColumn(\"distance_mi\",     F.col(\"trip_distance\"))\n",
    "          .withColumn(\"fare\",            F.col(\"total_amount\"))\n",
    "    )\n",
    "\n",
    "    agg = (df.groupBy(\"taxi_type\",\"pickup_date_id\",\"pickup_time_id\")\n",
    "              .agg(\n",
    "                  F.count(\"*\").alias(\"num_trips\"),\n",
    "                  F.sum(\"passenger_count\").alias(\"total_passengers\"),\n",
    "                  F.sum(\"fare\").alias(\"total_revenue\"),\n",
    "                  F.sum(\"distance_mi\").alias(\"distance_mi\"),\n",
    "                  F.sum(\"duration_min\").alias(\"total_duration\"))\n",
    "              .withColumn(\"avg_fare\",     F.round(F.col(\"total_revenue\")/F.col(\"num_trips\"),2))\n",
    "              .withColumn(\"avg_distance\", F.round(F.col(\"distance_mi\")/F.col(\"num_trips\"),2))\n",
    "              .withColumn(\"avg_duration\", F.round(F.col(\"total_duration\")/F.col(\"num_trips\"),2))\n",
    "              .withColumn(\"revenue_mi\",   F.round(F.col(\"total_revenue\")/F.col(\"distance_mi\"),2))\n",
    "              .withColumn(\"load_factor\",  F.round(F.col(\"total_passengers\")/F.col(\"num_trips\"),2))\n",
    "          )\n",
    "    agg_dfs.append(agg)\n",
    "\n",
    "all_agg = reduce(lambda a,b: a.unionByName(b, allowMissingColumns=True), agg_dfs)\n",
    "\n",
    "tgt_tbl = f\"{GOLD.SCHEMA}.fact_trips\"\n",
    "\n",
    "(all_agg.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")            \n",
    "    .partitionBy(\"pickup_date_id\") \n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .option(\"delta.feature.checkConstraints\",\"supported\")\n",
    "    .saveAsTable(tgt_tbl) \n",
    ")\n",
    "\n",
    "print(\"Gold fact_trips criada ‚Äî\",\n",
    "      spark.table(tgt_tbl).count(), \"linhas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646e3083-5a8a-477d-bb12-f9c6d0ded53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6965736245755702,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_etl_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
