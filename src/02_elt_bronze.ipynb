{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17828d18-442f-4fad-8098-08045bf2b837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c0da8c-dde1-43c1-b85e-ecb422eb375e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from dataclasses import dataclass, field\n",
    "from typing import FrozenSet, Tuple\n",
    "from functools import reduce\n",
    "import re\n",
    "from helpers import BRONZE, TAXI_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6989a27-b16a-4964-bc19-8b929911a5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d87dce-37a6-4ce5-95dd-39313cb3f029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ§¹ Leitura e padronizaÃ§Ã£o dos dados brutos (camada RAW â†’ BRONZE)\n",
    "\n",
    "FunÃ§Ãµes auxiliares para:\n",
    "\n",
    "- ðŸ” **Renomear colunas** para o padrÃ£o *snake_case* e uniformizar nomes como `airport_fee` e timestamps;\n",
    "- ðŸ§¼ **Normalizar colunas de data/hora**, ajustando possÃ­veis variaÃ§Ãµes entre datasets (`lpep_*` â†’ `tpep_*`);\n",
    "- ðŸ“ **Recursivamente listar e ler arquivos `.parquet`** do volume raw;\n",
    "- âš ï¸ **Validar colunas obrigatÃ³rias** e mover para a **quarentena** os arquivos com problemas.\n",
    "\n",
    "ðŸŽ¯ **Objetivo**: padronizar a estrutura e garantir qualidade mÃ­nima antes de promover os dados para a camada Bronze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77cd5430-51cb-4a48-8a13-9ab8ce6e70b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_snake(col_name:str) -> str:\n",
    "    name = re.sub(r'(.)([A-Z][a-z]+)',  r'\\1_\\2', col_name)\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "    return name.lower()\n",
    "\n",
    "def list_parquet_files(path):\n",
    "    out=[]\n",
    "    for e in dbutils.fs.ls(path):\n",
    "        if e.isDir():  out.extend(list_parquet_files(e.path))\n",
    "        elif e.path.endswith(\".parquet\"): out.append(e.path)\n",
    "    return out\n",
    "\n",
    "def normalize_datetime_cols(df):\n",
    "    mappings = {\n",
    "        \"lpep_pickup_datetime\" : \"tpep_pickup_datetime\",\n",
    "        \"lpep_dropoff_datetime\": \"tpep_dropoff_datetime\",\n",
    "    }\n",
    "    for src, dst in mappings.items():\n",
    "        if src in df.columns and dst not in df.columns:\n",
    "            df = df.withColumnRenamed(src, dst)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_one(p):\n",
    "    df = spark.read.parquet(p)\n",
    "\n",
    "    for old in df.columns:\n",
    "        new = to_snake(old)\n",
    "        if new != old:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "\n",
    "    if \"airport_fee\" not in df.columns and \"airportfee\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"airportfee\",\"airport_fee\")\n",
    "\n",
    "    for c in set(BRONZE.COLS_FORCE_DOUBLE).intersection(df.columns):\n",
    "        df = df.withColumn(c, F.col(c).cast(T.DoubleType()))\n",
    "\n",
    "    df = normalize_datetime_cols(df)\n",
    "    \n",
    "    if \"tpep_pickup_datetime\" in df.columns:\n",
    "        df = (df\n",
    "               .withColumn(\"trip_year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "               .withColumn(\"trip_month\", F.month(\"tpep_pickup_datetime\")))\n",
    "    return df\n",
    "\n",
    "def safe_read(path:str):\n",
    "    try:\n",
    "        df = load_one(path)\n",
    "        missing = [c for c in BRONZE.REQUIRED_COLS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"faltando cols {missing}\")\n",
    "        return df.withColumn(\"_source_file\", F.input_file_name())\n",
    "    except Exception as e:\n",
    "        print(\"Arquivo em quarentena:\", path.split('/')[-1], \"â€”\", e)\n",
    "        dbutils.fs.mv(path, f\"{BRONZE.QUARANTINE}/{path.split('/')[-1]}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4040eab1-3e85-4485-b665-53a4d3619fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸª™ ConstruÃ§Ã£o da camada Bronze (dados padronizados e validados)\n",
    "\n",
    "Este trecho:\n",
    "\n",
    "- ðŸ” Itera sobre os tipos de tÃ¡xi e carrega os arquivos vÃ¡lidos da camada `raw`;\n",
    "- ðŸ§ª **Valida e concatena** os dados, aplicando constraints como:\n",
    "  - `total_amount >= 0`\n",
    "  - `pickup <= dropoff`\n",
    "  - `trip_year = 2023`, entre outros;\n",
    "- ðŸ—ƒï¸ Escreve os dados em formato **Delta Lake**, particionando por `trip_year` e `trip_month`;\n",
    "- â™»ï¸ Arquivos invÃ¡lidos (schema inconsistente, colunas faltantes) sÃ£o movidos para **quarentena**.\n",
    "\n",
    "ðŸŽ¯ **Objetivo**: garantir que apenas dados minimamente confiÃ¡veis avancem para anÃ¡lises e transformaÃ§Ãµes mais refinadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2b8d8b-ecc9-4b3d-9b36-68374d030080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for tt in TAXI_TYPES:\n",
    "    src_dir = f\"{BRONZE.RAW_ROOT}/taxi_type={tt}\"\n",
    "    files   = list_parquet_files(src_dir)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Nenhum arquivo para {tt}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Bronze {tt} â€“ {len(files)} arquivos\")\n",
    "\n",
    "    dfs = [safe_read(p) for p in files]\n",
    "    dfs = [d for d in dfs if d is not None]\n",
    "    if not dfs:\n",
    "        print(f\"todos os arquivos de {tt} caÃ­ram na quarentena\")\n",
    "        continue\n",
    "\n",
    "    bronze_df = reduce(lambda d1, d2:\n",
    "                       d1.unionByName(d2, allowMissingColumns=True), dfs)\n",
    "\n",
    "    bronze_df = (bronze_df\n",
    "                 .withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "                 .withColumn(\"trip_year\",    F.year(\"tpep_pickup_datetime\"))\n",
    "                 .withColumn(\"trip_month\",   F.month(\"tpep_pickup_datetime\")))\n",
    "\n",
    "    for c in bronze_df.columns:\n",
    "        snake = to_snake(c)\n",
    "        if c != snake:\n",
    "            bronze_df = bronze_df.withColumnRenamed(c, snake)\n",
    "\n",
    "    invalid_cond = (\n",
    "        (F.col(\"total_amount\") < 0) |\n",
    "        (F.col(\"tpep_pickup_datetime\") > F.col(\"tpep_dropoff_datetime\")) |\n",
    "        (F.col(\"trip_year\")  != 2023) |\n",
    "        (~F.col(\"trip_month\").between(1, 12))\n",
    "    )\n",
    "\n",
    "    invalid_df = bronze_df.filter(invalid_cond)\n",
    "    if invalid_df.head(1):                                    \n",
    "        (invalid_df.write\n",
    "            .mode(\"append\")\n",
    "            .format(\"delta\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{BRONZE.QUARANTINE}/{tt}\"))\n",
    "        print(f\"{invalid_df.count()} linhas quarentenadas\")\n",
    "\n",
    "    bronze_df = bronze_df.filter(~invalid_cond)               \n",
    "\n",
    "    tbl = f\"{BRONZE.SCHEMA}.{tt}_tripdata_bronze\"\n",
    "\n",
    "    (bronze_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"trip_year\", \"trip_month\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .option(\"delta.feature.checkConstraints\", \"supported\")\n",
    "        .option(\"delta.constraints.total_amount_nonnegative\",\n",
    "                \"total_amount >= 0\")\n",
    "        .option(\"delta.constraints.pickup_before_dropoff\",\n",
    "                \"tpep_pickup_datetime <= tpep_dropoff_datetime\")\n",
    "        .option(\"delta.constraints.valid_year\",  \"trip_year = 2023\")\n",
    "        .option(\"delta.constraints.valid_month\", \"trip_month BETWEEN 1 AND 12\")\n",
    "        .saveAsTable(tbl))\n",
    "\n",
    "    print(\"Arquivo gravado â€”\", spark.table(tbl).count(), \"linhas\")\n",
    "\n",
    "print(\"Camada Bronze concluÃ­da\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6965736245755673,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_elt_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
