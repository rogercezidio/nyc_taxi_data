# ğŸŒŸ iFood Case - Data Architecture

Este repositÃ³rio apresenta a soluÃ§Ã£o desenvolvida para o desafio tÃ©cnico de Data Architect do iFood, com foco na ingestÃ£o, processamento e anÃ¡lise de dados de corridas de tÃ¡xi da cidade de Nova York, utilizando a stack do Databricks com Delta Lake e Unity Catalog.

---

## ğŸ  VisÃ£o Geral da SoluÃ§Ã£o

A soluÃ§Ã£o segue a abordagem **medalhÃ£o (raw â†’ bronze â†’ silver â†’ gold)** com validaÃ§Ãµes e enriquecimentos em cada etapa:

1. **IngestÃ£o:** download dos arquivos Parquet do site da NYC TLC (Jan-Mai/2023) e salva em volume RAW.
2. **Bronze:** padroniza colunas, valida esquema, aplica castings e descarta arquivos invÃ¡lidos para quarentena.
3. **Silver:** aplica filtros de qualidade, enriquecimentos temporais e restriÃ§Ã£o ao perÃ­odo vÃ¡lido.
4. **Gold:** cria uma dimensÃ£o de datas e uma tabela fato agregada com mÃ©tricas de corridas.

---

## ğŸ“Š Tecnologias Utilizadas

- **Databricks (com Unity Catalog)**
- **Delta Lake**
- **PySpark / Spark SQL**
- **Volumes UC (Managed e External)**
- **Python 3.10+**
- **dotenv + dbutils.secrets**

---

## ğŸ”— Estrutura do RepositÃ³rio

```text
ifood-case/
â”œâ”€ src/               # CÃ³digo-fonte da pipeline e configuraÃ§Ãµes
â”œâ”€ analysis/          # Notebooks com queries e insights
â”œâ”€ requirements.txt   # DependÃªncias para execuÃ§Ã£o local
â””â”€ README.md          # DocumentaÃ§Ã£o da soluÃ§Ã£o
```

---

## ğŸ“† Dados Utilizados

Fonte oficial da [NYC Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

PerÃ­odo considerado: **Janeiro a Maio de 2023**

Tipos de tÃ¡xi processados:
- `yellow`
- `green`
- `fhv`
- `fhvhv`

---

## ğŸ“ƒ Tabelas Criadas

### DimensÃµes
- `gold.dim_date`: dimensÃ£o de datas (2023)

### Fatos
- `gold.fact_trips`: mÃ©tricas agregadas por data, hora e tipo de tÃ¡xi

### IntermediÃ¡rias
- `bronze.<type>_tripdata_bronze`
- `silver.<type>_tripdata_silver`

---

## ğŸ§¹ Perguntas Respondidas (analysis/)

1. **MÃ©dia de `total_amount` por mÃªs (Yellow Taxis)**
2. **MÃ©dia de `passenger_count` por hora do dia (Maio/2023)**

---

## âš™ï¸ ExecuÃ§Ã£o 

1. Configure um cluster no Databricks com **Unity Catalog habilitado**.
2. No AWS IAM, crie uma **role com permissÃ£o para acessar seu bucket S3** e confianÃ§a na conta Databricks. Depois, crie a Storage Credential com:

   ```sql
   CREATE STORAGE CREDENTIAL minha_credencial_uc
   WITH IAM_ROLE = 'arn:aws:iam::<seu_account_id>:role/<sua_role>'
   COMMENT 'Credencial UC para acesso ao bucket';
   ```
   Se preferir use Cloudformation:

   ğŸ”— ReferÃªncia: https://docs.databricks.com/aws/pt/connect/unity-catalog/cloud-storage/external-locations

3. Crie um arquivo `.env` na raiz com as variÃ¡veis:

   ```env
   BUCKET_BASE=s3://seu-bucket/path
   TAG=nyc
   CATALOG=ifood_case
   CREDS_NAME=minha_credencial_uc
   ```

4. Execute os notebooks na ordem:

   1. `01_ingertion_raw`
   2. `02_etl_bronze`
   3. `03_etl_silver`
   4. `04_etl_gold`
---

## ğŸ“Š Extras
- Arquivos com problemas sÃ£o movidos automaticamente para a camada `quarentine`.
- Todas as tabelas Delta possuem **constraints de integridade** para garantir consistÃªncia.

---

## ğŸ™Œ ConsideraÃ§Ãµes Finais

A soluÃ§Ã£o Ã© escalÃ¡vel, auditÃ¡vel e aproveita boas prÃ¡ticas de engenharia de dados, como separaÃ§Ã£o por camadas, controle de qualidade, uso de particionamento e enriquecimento semÃ¢ntico.

Sinta-se livre para explorar os notebooks e adaptar para outros cenÃ¡rios analÃ­ticos!
